{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Personalized Medicine: Redefining Cancer Treatment - Kaggle Competition\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook describes my approach to the Kaggle Competition named in the title, which was a research cooperation with the Memorial Sloan Kettering Cancer Center (MSKCC).\n",
    "\n",
    "The goal of the competition was to create a machine learning algorithm that can classify genetic variations that are present in cancer cells. Tumors contain cells with many different abnormal mutations in their DNA: some of these mutations are the drivers of tumor growth, whereas others are neutral and considered *passengers*. Normally, mutations are manually classified into different categories after literature review by clinicians. The dataset made available for this competition contains mutations that have been manually anotated into 9 different categories. The goal is to predict the correct categories of mutations in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data comes in two different kinds of files: one of them contains information about the genetic variants (training_variants.csv and test_variants.csv) and the other contains the text (clinical evidence) that was used to manually classify the variants (training_text.csv and test_text.csv). The training data contains a class target feature corresponding to one of the 9 categories that variants can be classified as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Read Data \"\"\"\n",
    "train_variant = pd.read_csv(\"input/training_variants\")\n",
    "test_variant = pd.read_csv(\"input/stage2_test_variants.csv\")\n",
    "train_text = pd.read_csv(\"input/training_text\", sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "test_text = pd.read_csv(\"input/stage2_test_text.csv\", header=None, skiprows=1, names=[\"ID\", \"Text\"])\n",
    "train = pd.merge(train_variant, train_text, how='left', on='ID')\n",
    "train_y = train['Class'].values\n",
    "train_x = train.drop('Class', axis=1)\n",
    "train_size=len(train_x)\n",
    "print('Number of training variants: %d' % (train_size))\n",
    "# number of train data : 3321\n",
    "\n",
    "test_x = pd.merge(test_variant, test_text, how='left', on='ID')\n",
    "test_size=len(test_x)\n",
    "print('Number of test variants: %d' % (test_size))\n",
    "# number of test data : 5668\n",
    "\n",
    "test_index = test_x['ID'].values\n",
    "all_data = np.concatenate((train_x, test_x), axis=0)\n",
    "all_data = pd.DataFrame(all_data)\n",
    "all_data.columns = [\"ID\", \"Gene\", \"Variation\", \"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preprocessing\n",
    "\n",
    "To be able to use the data for machine learning, we need to extract the features from the dataset. This means that we have to transform the text data into vectors that can be understood by an algorithm. As I am not an expert in Natural Language Processing, I applied a modified version of [this script published on Kaggle.](https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing script by Aly Osama https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim import utils\n",
    "\n",
    "def constructLabeledSentences(data):\n",
    "    sentences=[]\n",
    "    for index, row in data.iteritems():\n",
    "        sentences.append(LabeledSentence(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n",
    "    return sentences\n",
    "\n",
    "def textClean(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", str(text))\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]    \n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "    \n",
    "def cleanup(text):\n",
    "    text = textClean(text)\n",
    "    text= text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    return text\n",
    "\n",
    "allText = all_data['Text'].apply(cleanup)\n",
    "sentences = constructLabeledSentences(allText)\n",
    "allText.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "Text_INPUT_DIM=300\n",
    "\n",
    "\n",
    "text_model=None\n",
    "filename='docEmbeddings_5_clean.d2v'\n",
    "if os.path.isfile(filename):\n",
    "    text_model = Doc2Vec.load(filename)\n",
    "else:\n",
    "    text_model = Doc2Vec(min_count=1, window=5, size=Text_INPUT_DIM, sample=1e-4, negative=5, workers=4, iter=5,seed=1)\n",
    "    text_model.build_vocab(sentences)\n",
    "    text_model.train(sentences, total_examples=text_model.corpus_count, epochs=text_model.iter)\n",
    "    text_model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_arrays = np.zeros((train_size, Text_INPUT_DIM))\n",
    "text_test_arrays = np.zeros((test_size, Text_INPUT_DIM))\n",
    "\n",
    "for i in range(train_size):\n",
    "    text_train_arrays[i] = text_model.docvecs['Text_'+str(i)]\n",
    "\n",
    "j=0\n",
    "for i in range(train_size,train_size+test_size):\n",
    "    text_test_arrays[j] = text_model.docvecs['Text_'+str(i)]\n",
    "    j=j+1\n",
    "    \n",
    "print(text_train_arrays[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "Gene_INPUT_DIM=25\n",
    "\n",
    "svd = TruncatedSVD(n_components=25, n_iter=Gene_INPUT_DIM, random_state=12)\n",
    "\n",
    "one_hot_gene = pd.get_dummies(all_data['Gene'])\n",
    "truncated_one_hot_gene = svd.fit_transform(one_hot_gene.values)\n",
    "\n",
    "one_hot_variation = pd.get_dummies(all_data['Variation'])\n",
    "truncated_one_hot_variation = svd.fit_transform(one_hot_variation.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_y)\n",
    "encoded_y = np_utils.to_categorical((label_encoder.transform(train_y)))\n",
    "print(encoded_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=np.hstack((truncated_one_hot_gene[:train_size],truncated_one_hot_variation[:train_size],text_train_arrays))\n",
    "test_set=np.hstack((truncated_one_hot_gene[train_size:],truncated_one_hot_variation[train_size:],text_test_arrays))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with a consolidated training set and test set that look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set shape is: ', train_set.shape)\n",
    "print('Test set shape is: ', test_set.shape)\n",
    "\n",
    "print('Training set example rows:')\n",
    "print(train_set[0][:10])\n",
    "\n",
    "print('Test set example rows:')\n",
    "print(test_set[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a 4-layer neural network for classification\n",
    "\n",
    "The next step is to create a neural network on TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "dirname = 'output/'  # output directory\n",
    "filename = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_set, encoded_y, test_size=0.20, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = X_train.T, X_val.T, Y_train.T, Y_val.T\n",
    "\n",
    "# transpose test set\n",
    "X_test = test_set.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view data set shapes\n",
    "print('X_train: ', X_train.shape)\n",
    "print('X_val: ', X_val.shape)\n",
    "print('Y_train: ', Y_train.shape)\n",
    "print('Y_val: ', Y_val.shape)\n",
    "print('X_test: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "\n",
    "    Arguments:\n",
    "    n_x -- scalar, dimensions of the input\n",
    "    n_y -- scalar, number of classes (from 0 to 8, so -> 9)\n",
    "\n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(n_x, None), name='X')\n",
    "    Y = tf.placeholder(tf.float32, shape=(n_y, None), name='Y')\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow.\n",
    "\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W and b for every layer\n",
    "    \"\"\"\n",
    "\n",
    "    tf.set_random_seed(1)\n",
    "\n",
    "    W1 = tf.get_variable('W1', [350, X_train.shape[0]], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b1 = tf.get_variable('b1', [350, 1], initializer=tf.zeros_initializer())\n",
    "    W2 = tf.get_variable('W2', [350, 350], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b2 = tf.get_variable('b2', [350, 1], initializer=tf.zeros_initializer())\n",
    "    W3 = tf.get_variable('W3', [100, 350], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b3 = tf.get_variable('b3', [100, 1], initializer=tf.zeros_initializer())\n",
    "    W4 = tf.get_variable('W4', [9, 100], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b4 = tf.get_variable('b4', [9, 1], initializer=tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                  \"W4\": W4,\n",
    "                  \"b4\": b4}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, keep_prob1, keep_prob2):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: (LINEAR -> RELU)^3 -> LINEAR -> SOFTMAX\n",
    "\n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W\" and \"b\" for every layer\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z4 -- the output of the last LINEAR unit (logits)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the parameters from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "\n",
    "    Z1 = tf.matmul(W1, X) + b1  # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)  # A1 = relu(Z1)\n",
    "    A1 = tf.nn.dropout(A1, keep_prob1)  # add dropout\n",
    "    Z2 = tf.matmul(W2, A1) + b2  # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)  # A2 = relu(Z2)\n",
    "    A2 = tf.nn.dropout(A2, keep_prob2)  # add dropout\n",
    "    Z3 = tf.matmul(W3, A2) + b3  # Z3 = np.dot(W3,Z2) + b3\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    Z4 = tf.matmul(W4, A3) + b4\n",
    "\n",
    "    return Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z4, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "\n",
    "    Arguments:\n",
    "    Z4 -- output of forward propagation (output of the last LINEAR unit), of shape (n_classes, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z4\n",
    "\n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "\n",
    "    # transpose to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z4)\n",
    "    labels = tf.transpose(Y)\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size, seed=0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector, of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0], m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(\n",
    "        m / mini_batch_size)  # number of mini batches of size mini_batch_size in your partitioning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    W1 = tf.convert_to_tensor(parameters['W1'])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
    "    W4 = tf.convert_to_tensor(parameters[\"W4\"])\n",
    "    b4 = tf.convert_to_tensor(parameters[\"b4\"])\n",
    "\n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3,\n",
    "              \"W4\": W4,\n",
    "              \"b4\": b4}\n",
    "\n",
    "    x = tf.placeholder(\"float\", [X_train.shape[0], None])\n",
    "    keep_prob1 = tf.placeholder(tf.float32, name='keep_prob1')\n",
    "    keep_prob2 = tf.placeholder(tf.float32, name='keep_prob2')\n",
    "\n",
    "    z4 = forward_propagation(x, params, keep_prob1, keep_prob2)\n",
    "    p = tf.nn.softmax(z4, dim=0)  # dim=0 because the classes are on that axis\n",
    "    # p = tf.argmax(z4) # this gives only the predicted class as output\n",
    "\n",
    "    sess = tf.Session()\n",
    "    prediction = sess.run(p, feed_dict={x: X, keep_prob1: 1.0, keep_prob2: 1.0})\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate=0.0001,\n",
    "          num_epochs=1000, minibatch_size=64, print_cost=True):\n",
    "    \"\"\"\n",
    "    Implements a four-layer tensorflow neural network: (LINEAR->RELU)^3->LINEAR->SOFTMAX.\n",
    "\n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size, number of training examples)\n",
    "    Y_train -- test set, of shape (output size, number of training examples)\n",
    "    X_test -- training set, of shape (input size, number of training examples)\n",
    "    Y_test -- test set, of shape (output size, number of test examples)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    ops.reset_default_graph()  # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)  # to keep consistent results\n",
    "    seed = 3  # to keep consistent results\n",
    "    (n_x, m) = X_train.shape  # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]  # n_y : output size\n",
    "    costs = []  # To keep track of the cost\n",
    "    t0 = time.time()  # to mark the start of the training\n",
    "\n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    keep_prob1 = tf.placeholder(tf.float32, name='keep_prob1')  # probability to keep a unit during dropout\n",
    "    keep_prob2 = tf.placeholder(tf.float32, name='keep_prob2')\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    # Forward propagation\n",
    "    Z4 = forward_propagation(X, parameters, keep_prob1, keep_prob2)\n",
    "\n",
    "    # Cost function\n",
    "    cost = compute_cost(Z4, Y)\n",
    "    regularizers = tf.nn.l2_loss(parameters['W1']) + tf.nn.l2_loss(parameters['W2']) + tf.nn.l2_loss(parameters['W3']) \\\n",
    "                   + tf.nn.l2_loss(parameters['W4'])  # add regularization term\n",
    "    beta = 0.01  # regularization constant\n",
    "    cost = tf.reduce_mean(cost + beta * regularizers)  # cost with regularization\n",
    "\n",
    "    # Backpropagation: Define the tensorflow AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.  # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size)  # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\"\n",
    "                _, minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y,\n",
    "                                                                           keep_prob1: 0.7, keep_prob2: 0.5})\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print(\"Cost after epoch {}: {:f}\".format(epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z4), tf.argmax(Y))\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        train_cost = cost.eval({X: X_train, Y: Y_train, keep_prob1: 1.0, keep_prob2: 1.0})\n",
    "        test_cost = cost.eval({X: X_test, Y: Y_test, keep_prob1: 1.0, keep_prob2: 1.0})\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train, keep_prob1: 1.0, keep_prob2: 1.0})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test, keep_prob1: 1.0, keep_prob2: 1.0})\n",
    "\n",
    "        print('Finished training in %s s' % (time.time() - t0))\n",
    "        print(\"Train Cost:\", train_cost)\n",
    "        print(\"Test Cost:\", test_cost)\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per fives)')\n",
    "        plt.title(\"Learning rate = {}, beta = {},\\n\"\n",
    "                  \"test cost = {:.6f}, test accuracy = {:.6f}\".format(learning_rate, beta, test_cost, test_accuracy))\n",
    "        global filename\n",
    "        filename = timestr + '_NN4Lstage2_lr_{}_beta_{}_cost_{:.2f}-{:.2f}_acc_{:.2f}-{:.2f}'.format(\n",
    "            learning_rate, beta, train_cost, test_cost, train_accuracy, test_accuracy)\n",
    "        plt.savefig(dirname + filename + '.png')\n",
    "\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model and get learned parameters\n",
    "parameters = model(X_train, Y_train, X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use learned parameters to make prediction on test data\n",
    "prediction = predict(X_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission file\n",
    "submission = pd.DataFrame(prediction.T)\n",
    "submission['id'] = test_index\n",
    "submission.columns = ['class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7', 'class8', 'class9', 'id']\n",
    "submission.to_csv(dirname + filename + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
